{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f2ca62f-6d05-480b-bf90-e89a9ab13ca1",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install dioptra"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2b50328-575c-48a1-b130-ba86cd50ae70",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install datasets torchvision transformers evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3be5400e-fcfe-42a8-9d9f-6ff8c352dbc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "####\n",
    "#\n",
    "# Fetch an open source dataset from the Huggingface Hub\n",
    "#\n",
    "####\n",
    "\n",
    "from datasets import load_dataset\n",
    "import random\n",
    "\n",
    "cats_vs_gods_dataset = load_dataset('cats_vs_dogs')['train']\n",
    "\n",
    "groundtruth = list(cats_vs_gods_dataset['labels'])\n",
    "\n",
    "all_indexes = list(range(0, len(cats_vs_gods_dataset)))\n",
    "random.Random(1234).shuffle(all_indexes)\n",
    "\n",
    "training_indexes = all_indexes[0: int(0.8 * len(all_indexes))]\n",
    "testing_indexes = all_indexes[int(0.8 * len(all_indexes)): -1]\n",
    "training_indexes.sort()\n",
    "testing_indexes.sort()\n",
    "\n",
    "reverse_ontology = {\n",
    "  0: \"cat\",\n",
    "  1: \"dog\"\n",
    "}\n",
    "\n",
    "ontology = {\n",
    "  \"cat\": 0,\n",
    "  \"dog\": 1\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae980d4a-7cbf-4ff4-a1fb-b88a87ace2bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "####\n",
    "#\n",
    "# Set some env variables\n",
    "#\n",
    "####\n",
    "\n",
    "import os\n",
    "img_bucket = 'my_bucket'\n",
    "img_dir = f's3://{img_bucket}/end_to_end_test/imgs'\n",
    "\n",
    "os.environ['DIOPTRA_API_KEY'] = 'my_api_key'\n",
    "os.environ['DIOPTRA_UPLOAD_BUCKET'] = img_bucket\n",
    "os.environ['DIOPTRA_UPLOAD_PREFIX'] = 'end_to_end_test/logs/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf82cc24-55c7-4c6f-905d-d2b72aef930a",
   "metadata": {},
   "outputs": [],
   "source": [
    "####\n",
    "#\n",
    "# Define some labeling provider\n",
    "#\n",
    "####\n",
    "\n",
    "class LabelProvider():\n",
    "    def __init__(self, groundtruth, reverse_ontology):\n",
    "        self.groundtruth = groundtruth\n",
    "        self.reverse_ontology = reverse_ontology\n",
    "\n",
    "    def label_data(self, dataframe):\n",
    "        labels = []\n",
    "        for index, row in dataframe.iterrows():\n",
    "            dataset_index = row['tags.datapoint_id']\n",
    "            label = self.reverse_ontology[self.groundtruth[dataset_index]]\n",
    "            labels.append(label)\n",
    "        dataframe['groundtruth.class_name'] = labels\n",
    "        return dataframe\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2171a392-afe8-497b-ac2c-f6826f0ccaaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "####\n",
    "#\n",
    "# Upload the imgs to S3\n",
    "#\n",
    "####\n",
    "\n",
    "import os\n",
    "os.mkdir('imgs')\n",
    "for index in range(len(all_indexes)):\n",
    "    cats_vs_gods_dataset[index]['image'].save(f'./imgs/{index}.jpg', format='jpeg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca2b2c3a-0818-4733-a6ba-e2ff06b98cf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "!aws s3 cp imgs {img_dir} --recursive --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d0b3785-96b4-4a23-9af2-2da540d8cbbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "####\n",
    "#\n",
    "# Upload the metadata to dioptra\n",
    "#\n",
    "####\n",
    "\n",
    "from dioptra.lake.utils import upload_to_lake\n",
    "\n",
    "initial_metadata = []\n",
    "for index in range(len(all_indexes)):\n",
    "    initial_metadata.append({\n",
    "        'image_metadata': {\n",
    "            'uri': f'{img_dir}/{index}.jpg'\n",
    "        },\n",
    "        'tags': {\n",
    "            'datapoint_id': index,\n",
    "            'data_split': 'train' if index in training_indexes else 'test'\n",
    "        }})\n",
    "\n",
    "for batch in [initial_metadata[i:i + 1000] for i in range(0, len(initial_metadata), 1000)]:\n",
    "    upload_to_lake(batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63b4c259-8c6e-4c6f-aea4-40fb88d729a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "####\n",
    "#\n",
    "# Create a test dataset\n",
    "#\n",
    "####\n",
    "\n",
    "from dioptra.lake.utils import download_from_lake\n",
    "from dioptra.lake.datasets import Dataset as DioptraDataset\n",
    "\n",
    "test_df = download_from_lake(filters=[{\n",
    "    'left': 'tags.data_split',\n",
    "    'op': '=',\n",
    "    'right': 'test'\n",
    "}], fields=['uuid', 'request_id', 'tags.datapoint_id'])\n",
    "\n",
    "\n",
    "test_dataset = DioptraDataset()\n",
    "test_dataset.create('test_cast_vs_dogs')\n",
    "test_dataset.add(list(test_df['uuid']))\n",
    "test_dataset.commit('initial commit')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96de25e5-11ee-4923-b3fd-7353036df739",
   "metadata": {},
   "outputs": [],
   "source": [
    "####\n",
    "#\n",
    "# Get the data labled by some provider and update the lake with the new groundtruth\n",
    "#\n",
    "####\n",
    "\n",
    "my_label_provider = LabelProvider(groundtruth, reverse_ontology)\n",
    "test_df = my_label_provider.label_data(test_df)\n",
    "\n",
    "update_dataset = []\n",
    "\n",
    "for index, row in test_df.iterrows():\n",
    "    update_dataset.append({'request_id': row['request_id'], 'groundtruth': {'class_name': row['groundtruth.class_name']}})\n",
    "    \n",
    "for batch in [update_dataset[i:i + 1000] for i in range(0, len(update_dataset), 1000)]:\n",
    "    upload_to_lake(batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d34a7f38-809d-4af5-8864-a164c6fb9b10",
   "metadata": {},
   "outputs": [],
   "source": [
    "####\n",
    "#\n",
    "# Download training unlabeled data as a dataset\n",
    "#\n",
    "####\n",
    "\n",
    "from dioptra.lake.utils import download_from_lake\n",
    "from dioptra.lake.torch.object_store_datasets import ImageDataset\n",
    "\n",
    "unlabeled_df = download_from_lake(filters=[{\n",
    "    'left': 'tags.data_split',\n",
    "    'op': '=',\n",
    "    'right': 'train'\n",
    "}], fields=['image_metadata.uri', 'tags.datapoint_id', 'request_id'])\n",
    "\n",
    "unlabeled_dataset = ImageDataset(unlabeled_df)\n",
    "\n",
    "first_run_metadata = []\n",
    "for row in unlabeled_dataset:\n",
    "    first_run_metadata.append({'request_id': row['request_id'], 'tags': {'run_id': 'initial'}})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cd4fecd-83a5-4e1f-9d57-d09028da3639",
   "metadata": {},
   "outputs": [],
   "source": [
    "####\n",
    "#\n",
    "# Let's define the transform pipe and pre fetch the images (optional)\n",
    "#\n",
    "####\n",
    "\n",
    "import io\n",
    "import smart_open\n",
    "import torch\n",
    "from torchvision import transforms\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from dioptra.inference.torch.classifier_runner import ClassifierRunner\n",
    "\n",
    "transform_pipe = transforms.Compose([\n",
    "    transforms.Lambda(lambda x: x.convert('RGB')),\n",
    "    transforms.Resize(256),\n",
    "    transforms.CenterCrop(224),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Lambda(lambda x: x.repeat(3, 1, 1) if x.shape[0] == 1 else x),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "def transform(row):\n",
    "    return transform_pipe(row['image'])\n",
    "    \n",
    "unlabeled_dataset.transform = transform\n",
    "unlabeled_dataset.load_images = True\n",
    "unlabeled_dataset.prefetch_images(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf78bf9d-ea9e-4598-b2df-a077cbbcbddc",
   "metadata": {},
   "outputs": [],
   "source": [
    "####\n",
    "#\n",
    "# Use a pre trained model to generate first embeddings\n",
    "# Let's use a plain torch model for this one\n",
    "#\n",
    "####\n",
    "\n",
    "data_loader = DataLoader(\n",
    "    unlabeled_dataset, batch_size=10, num_workers=4, shuffle=False)\n",
    "\n",
    "torch_model = torch.hub.load('pytorch/vision:v0.10.0', 'resnet18', pretrained=True)\n",
    "torch_model.to('cuda')\n",
    "\n",
    "my_runner = ClassifierRunner(\n",
    "    model=torch_model, \n",
    "    embeddings_layers=['layer4'],\n",
    "    device='cuda',\n",
    "    metadata=first_run_metadata\n",
    ")\n",
    "\n",
    "my_runner.run(data_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7782b67c-17c5-4730-9a3a-b6e727d8edc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "####\n",
    "#\n",
    "# Check that we got all the data ingested\n",
    "#\n",
    "####\n",
    "\n",
    "download_from_lake(\n",
    "    filters=[{'left': 'tags.run_id', 'op': '=', 'right': 'initial'}],\n",
    "    fields=['uuid', 'request_id', 'tags.datapoint_id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b168030d-1163-4fff-856b-60edcaa924b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "####\n",
    "#\n",
    "# Kick off a coreset miner to pull the first 100 samples\n",
    "#\n",
    "####\n",
    "\n",
    "\n",
    "import time\n",
    "from dioptra.miners.coreset_miner import CoresetMiner\n",
    "\n",
    "my_miner = CoresetMiner(\n",
    "    select_filters=[{\n",
    "        'left': 'tags.run_id',\n",
    "        'op': '=',\n",
    "        'right': 'initial'}],\n",
    "    size=100,\n",
    "    display_name='coreset miner',\n",
    "    embeddings_field='embeddings',\n",
    "    skip_caching=True\n",
    ")\n",
    "\n",
    "my_miner.run()\n",
    "\n",
    "while my_miner.get_status() != 'SUCCESS':\n",
    "    print('waiting for results')\n",
    "    time.sleep(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87b8e05d-e7dd-43bc-9d4c-7f491bf0af61",
   "metadata": {},
   "outputs": [],
   "source": [
    "####\n",
    "#\n",
    "# Let's create our training dataset and add the miner results to the dataset\n",
    "#\n",
    "####\n",
    "\n",
    "from dioptra.lake.datasets import Dataset as DioptraDataset\n",
    "\n",
    "my_dataset = DioptraDataset()\n",
    "my_dataset.create('training_cast_vs_dogs')\n",
    "\n",
    "coreset_results_df = download_from_lake(\n",
    "    filters=[{'left': 'uuid', 'op': 'in', 'right': my_miner.get_results()}],\n",
    "    fields=['uuid', 'request_id', 'tags.datapoint_id'])\n",
    "\n",
    "my_dataset.add(list(coreset_results_df['uuid']))\n",
    "my_dataset.commit('initial_version')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "622aaae7-d73e-4357-9142-40e8e7f46731",
   "metadata": {},
   "outputs": [],
   "source": [
    "####\n",
    "#\n",
    "# Use our labeling provider to labels the data and update the lake\n",
    "#\n",
    "####\n",
    "\n",
    "from dioptra.lake.utils import upload_to_lake\n",
    "\n",
    "my_label_provider = LabelProvider(groundtruth, reverse_ontology)\n",
    "\n",
    "labeled_df = my_label_provider.label_data(coreset_results_df)\n",
    "\n",
    "update_dataset = []\n",
    "\n",
    "for index, row in labeled_df.iterrows():\n",
    "    update_dataset.append({'request_id': row['request_id'], 'groundtruth': {'class_name': row['groundtruth.class_name']}})\n",
    "    \n",
    "for batch in [update_dataset[i:i + 1000] for i in range(0, len(update_dataset), 1000)]:\n",
    "    upload_to_lake(batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c8306e7-3f47-447b-b9b4-cb51e888bf38",
   "metadata": {},
   "outputs": [],
   "source": [
    "####\n",
    "#\n",
    "# Let's download and prep our training dataset\n",
    "#\n",
    "####\n",
    "\n",
    "new_training_df = my_dataset.download(fields=['image_metadata.uri', 'groundtruth.class_name'])\n",
    "new_training_df = new_training_df[new_training_df['groundtruth.class_name'] != '']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25995709-09e0-4cb4-9098-88619451ed66",
   "metadata": {},
   "outputs": [],
   "source": [
    "####\n",
    "#\n",
    "# our training routine ...\n",
    "#\n",
    "####\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "import evaluate\n",
    "from torchvision import transforms\n",
    "\n",
    "from transformers import (\n",
    "    AutoConfig,\n",
    "    AutoImageProcessor,\n",
    "    AutoModelForImageClassification,\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    "    EarlyStoppingCallback\n",
    ")\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from dioptra.lake.torch.object_store_datasets import ImageDataset\n",
    "\n",
    "config = AutoConfig.from_pretrained(\n",
    "    'microsoft/resnet-50',\n",
    "    num_labels=len(ontology),\n",
    "    label2id=reverse_ontology,\n",
    "    id2label=ontology,\n",
    "    finetuning_task=\"image-classification\"\n",
    ")\n",
    "model = AutoModelForImageClassification.from_pretrained(\n",
    "    'microsoft/resnet-50',\n",
    "    ignore_mismatched_sizes=True,\n",
    "    config=config\n",
    ")\n",
    "image_processor = AutoImageProcessor.from_pretrained(\n",
    "    'microsoft/resnet-50'\n",
    ")\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='test_trainer',\n",
    "    evaluation_strategy='epoch',\n",
    "    logging_strategy='epoch',\n",
    "    save_strategy='epoch',\n",
    "    num_train_epochs=20,\n",
    "    learning_rate=5e-4,\n",
    "    load_best_model_at_end = True\n",
    ")\n",
    "\n",
    "metric = evaluate.load('accuracy')\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "    return metric.compute(predictions=predictions, references=labels)\n",
    "\n",
    "def collate_fn(examples):\n",
    "    pixel_values = torch.stack([example['pixel_values'] for example in examples])\n",
    "    labels = torch.tensor([example['labels'] for example in examples])\n",
    "    return {'pixel_values': pixel_values, 'labels': labels}\n",
    "\n",
    "_train_transforms = transforms.Compose(\n",
    "    [\n",
    "        transforms.Lambda(lambda x: x.convert('RGB')),\n",
    "        transforms.RandomResizedCrop((image_processor.size['shortest_edge'])),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=image_processor.image_mean, std=image_processor.image_std),\n",
    "    ]\n",
    ")\n",
    "_eval_transforms = transforms.Compose(\n",
    "    [\n",
    "        transforms.Lambda(lambda x: x.convert('RGB')),\n",
    "        transforms.Resize(image_processor.size['shortest_edge']),\n",
    "        transforms.CenterCrop((image_processor.size['shortest_edge'])),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=image_processor.image_mean, std=image_processor.image_std),\n",
    "    ]\n",
    ")\n",
    "\n",
    "\n",
    "def train_transforms(example_batch):\n",
    "    return {\n",
    "        'pixel_values': _train_transforms(example_batch['image']),\n",
    "        'labels': ontology[example_batch['groundtruth.class_name']]\n",
    "    }\n",
    "\n",
    "def eval_transforms(example_batch):\n",
    "    return {\n",
    "        'pixel_values': _eval_transforms(example_batch['image']),\n",
    "        'labels': ontology[example_batch['groundtruth.class_name']]\n",
    "    }\n",
    "\n",
    "new_training_df = new_training_df.sample(frac=1).reset_index(drop=True)\n",
    "\n",
    "training_data = ImageDataset(\n",
    "    dataframe=new_training_df.iloc[0: int(len(new_training_df) * 0.6)],\n",
    "    transform=train_transforms)\n",
    "evaluation_data = ImageDataset(\n",
    "    dataframe=new_training_df.iloc[int(len(new_training_df) * 0.6): -1],\n",
    "    transform=eval_transforms)\n",
    "\n",
    "training_data.load_images = True\n",
    "evaluation_data.load_images = True\n",
    "\n",
    "trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=training_data,\n",
    "        eval_dataset=evaluation_data,\n",
    "        compute_metrics=compute_metrics,\n",
    "        tokenizer=image_processor,\n",
    "        data_collator=collate_fn,\n",
    "        callbacks=[EarlyStoppingCallback(early_stopping_patience = 10)]\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1624ac3a-56e6-4f99-8e7b-118d3e651460",
   "metadata": {},
   "outputs": [],
   "source": [
    "####\n",
    "#\n",
    "# Start the training\n",
    "#\n",
    "####\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "290f36c6-2a24-440c-bb90-ac3b33d3475a",
   "metadata": {},
   "outputs": [],
   "source": [
    "####\n",
    "#\n",
    "# Get our test dataset\n",
    "#\n",
    "####\n",
    "\n",
    "from dioptra.lake.torch.object_store_datasets import ImageDataset\n",
    "\n",
    "test_dataset = DioptraDataset()\n",
    "test_dataset.dataset_id = '7c91b00a-02b3-4d1d-90fa-63e017bb4597'\n",
    "test_df = test_dataset.download()\n",
    "test_df = test_df[test_df['groundtruth.class_name'].notna()]\n",
    "test_data = ImageDataset(\n",
    "    dataframe=test_df,\n",
    "    transform=eval_transforms)\n",
    "test_data.prefetch_images(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f7e5cf3-f553-4a73-a5aa-0bb29e08e5d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "####\n",
    "#\n",
    "# Evaluate ...\n",
    "#\n",
    "####\n",
    "\n",
    "\n",
    "trainer.evaluate(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e74e5cee-a26e-42f5-8d80-54f69ca14059",
   "metadata": {},
   "outputs": [],
   "source": [
    "####\n",
    "#\n",
    "# Get the next batch of data\n",
    "#\n",
    "####\n",
    "\n",
    "\n",
    "from dioptra.lake.utils import download_from_lake\n",
    "from dioptra.lake.torch.object_store_datasets import ImageDataset\n",
    "\n",
    "second_run_df = download_from_lake(filters=[{\n",
    "    'left': 'tags.data_split',\n",
    "    'op': '=',\n",
    "    'right': 'train'\n",
    "}], fields=['image_metadata.uri', 'tags.datapoint_id', 'request_id']).drop_duplicates('request_id', keep='first')\n",
    "\n",
    "second_run_dataset = ImageDataset(second_run_df)\n",
    "\n",
    "second_run_metadata = []\n",
    "for row in second_run_dataset:\n",
    "    second_run_metadata.append({'request_id': row['request_id'], 'tags': {'run_id': 'second'}})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d39297bc-a0c7-4364-b35a-c6181cefcca4",
   "metadata": {},
   "outputs": [],
   "source": [
    "####\n",
    "#\n",
    "# Kick off a new run ...\n",
    "#\n",
    "####\n",
    "\n",
    "def my_transforms(example_batch):\n",
    "    return _eval_transforms(example_batch['image'])\n",
    "\n",
    "second_run_dataset.load_images = True\n",
    "second_run_dataset.transform = my_transforms\n",
    "\n",
    "second_run_data_loader = DataLoader(\n",
    "    second_run_dataset, batch_size=10, num_workers=4, shuffle=False)\n",
    "\n",
    "my_runner_2 = ClassifierRunner(\n",
    "    model=model, \n",
    "    embeddings_layers=['resnet.pooler'],\n",
    "    logits_layer='classifier',\n",
    "    device='cuda',\n",
    "    metadata=second_run_metadata,\n",
    "    class_names=list(ontology.keys())\n",
    ")\n",
    "\n",
    "my_runner_2.run(second_run_data_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdfc14db-347b-44f7-9e7f-1a8a23c20331",
   "metadata": {},
   "outputs": [],
   "source": [
    "####\n",
    "#\n",
    "# Start a new set of miners\n",
    "#\n",
    "####\n",
    "\n",
    "import time\n",
    "from dioptra.miners.activation_miner import ActivationMiner\n",
    "from dioptra.miners.coreset_miner import CoresetMiner\n",
    "from dioptra.miners.entropy_miner import EntropyMiner\n",
    "\n",
    "filters = [{\n",
    "    'left': 'tags.run_id',\n",
    "    'op': '=',\n",
    "    'right': 'second'\n",
    "}]\n",
    "\n",
    "current_training_filters = [{\n",
    "    'left': 'request_id',\n",
    "    'op': 'in',\n",
    "    'right': list(my_dataset.download()['request_id'])\n",
    "}]\n",
    "\n",
    "my_miners = []\n",
    "my_miners.append(ActivationMiner(\n",
    "    select_filters=filters,\n",
    "    size=33,\n",
    "    display_name='activation miner 2',\n",
    "    embeddings_field='embeddings',\n",
    "    skip_caching=True\n",
    "))\n",
    "\n",
    "my_miners.append(CoresetMiner(\n",
    "    select_filters=filters,\n",
    "    select_reference_filters=current_training_filters,\n",
    "    size=33,\n",
    "    display_name='coreset miner 2',\n",
    "    embeddings_field='embeddings',\n",
    "    skip_caching=True\n",
    "))\n",
    "\n",
    "my_miners.append(EntropyMiner(\n",
    "    select_filters=filters,\n",
    "    size=33,\n",
    "    display_name='entropy miner 2'\n",
    "))\n",
    "\n",
    "for miner in my_miners:\n",
    "    miner.run()\n",
    "                 \n",
    "\n",
    "done = False\n",
    "while not done:\n",
    "    print('waiting for results')\n",
    "    time.sleep(10)\n",
    "    for miner in my_miners:\n",
    "        if miner.get_status() != 'SUCCESS':\n",
    "            continue\n",
    "        done = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "951471bb-1c18-4fa3-be79-c967152b1a99",
   "metadata": {},
   "outputs": [],
   "source": [
    "####\n",
    "#\n",
    "# Add the new match to our training dataset\n",
    "#\n",
    "####\n",
    "\n",
    "for miner in my_miners:\n",
    "    results_df = download_from_lake(\n",
    "        filters=[{'left': 'uuid', 'op': 'in', 'right': miner.get_results()}],\n",
    "        fields=['uuid', 'request_id', 'tags.datapoint_id'])\n",
    "\n",
    "    my_dataset.add(list(results_df['uuid']))\n",
    "my_dataset.commit('second version')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03fc9318-c118-4d1e-82cd-cfd9f035bad7",
   "metadata": {},
   "outputs": [],
   "source": [
    "####\n",
    "#\n",
    "# Use our labeling provider to labels the data and update the lake\n",
    "#\n",
    "####\n",
    "\n",
    "my_label_provider = LabelProvider(groundtruth, reverse_ontology)\n",
    "\n",
    "for miner in my_miners:\n",
    "    results_df = download_from_lake(\n",
    "        filters=[{'left': 'uuid', 'op': 'in', 'right': miner.get_results()}],\n",
    "        fields=['uuid', 'request_id', 'tags.datapoint_id'])\n",
    "\n",
    "    labeled_df = my_label_provider.label_data(results_df)\n",
    "\n",
    "    update_dataset = []\n",
    "\n",
    "    for index, row in labeled_df.iterrows():\n",
    "        update_dataset.append({'request_id': row['request_id'], 'groundtruth': {'class_name': row['groundtruth.class_name']}})\n",
    "\n",
    "    for batch in [update_dataset[i:i + 1000] for i in range(0, len(update_dataset), 1000)]:\n",
    "        upload_to_lake(batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7e6d434-6af7-4262-9a01-79217519b4f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "####\n",
    "#\n",
    "# Continue ...\n",
    "#\n",
    "####"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_pytorch_p39",
   "language": "python",
   "name": "conda_pytorch_p39"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
